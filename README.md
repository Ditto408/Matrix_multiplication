
### 项目目录结构


```Plaintext
.
├── code/
│   ├── datagen.py          # [Python] 稀疏/稠密矩阵数据生成脚本
│   ├── matmul.dml          # [DML] SystemML 矩阵乘法核心脚本
│   ├── matrix_ed3.py       # [Python] PySpark 矩阵乘法实现 (含 Naive/Broadcast/Block)
│   ├── MatrixApp.scala     # [Scala] Scala 矩阵乘法实现 (仅 Block)
│   ├── run_experiments.sh  # [Shell] PySpark 实验自动化运行脚本
│   ├── run_gen.sh          # [Shell] 数据生成自动化脚本 (Local -> HDFS)
│   ├── run_scala.sh        # [Shell] Scala 实验自动化运行脚本
│   └── run_systemml.sh     # [Shell] SystemML 实验自动化运行脚本
└── README.md               # 项目说明文档
```

---

# 基于 Spark 的分布式矩阵乘法实现与优化

## 1. 研究目的

随着大数据与机器学习任务规模的不断增长，大规模矩阵乘法已成为分布式计算系统中的核心算子之一。如何在分布式环境下高效完成矩阵乘法计算，对于提升整体计算性能具有重要意义。

本实验以 Apache Spark 为基础计算平台，围绕分布式矩阵乘法的实现与优化展开研究，旨在：
* 探索在 Spark 框架下实现分布式矩阵乘法的不同策略及其性能特征；
* 分析矩阵规模和稀疏度变化对计算效率与系统资源消耗的影响；
* 通过实验对比 自定义 Spark 实现 与 SystemML 提供的矩阵乘法实现，理解通用系统优化与定制化实现之间的差异；
* 总结在大规模分布式矩阵计算中提升性能的有效工程优化方法，为后续相关应用提供实践参考。


## 2. 研究内容

围绕分布式矩阵乘法的实现与性能优化问题，本实验主要从实现方法设计、性能对比分析以及优化策略总结等方面开展研究，具体内容如下。

### 2.1 基于 Spark 的分布式矩阵乘法实现

在 Apache Spark 平台上，分别实现了多种分布式矩阵乘法方案，以对比不同计算策略在分布式环境下的性能特点。具体包括：

* 朴素矩阵乘法实现（Naive Implementation）

  采用基于坐标表示的矩阵存储方式，通过对共享维度进行 join 操作并在后续阶段完成结果聚合。该实现逻辑清晰、易于理解，但在计算过程中会产生大量中间结果，Shuffle 开销较大，扩展性较差。

* 基于广播变量的矩阵乘法实现（Broadcast Implementation）

  将其中一个矩阵以广播变量的形式分发到各个 Executor，在本地完成乘法计算，从而减少部分 Shuffle 操作。该方法在被广播矩阵规模较小时具有较好的性能表现，但当矩阵规模较大时，广播与序列化成本会显著增加。

* 基于分块策略的矩阵乘法实现（Block Matrix Multiplication）

  将矩阵划分为多个子块（Block），以块为单位进行矩阵乘法计算。分块实现中在块内引入稀疏矩阵存储格式，有效降低了内存占用，并显著减少了 Shuffle 阶段传输的数据规模。该方法在大规模和稀疏矩阵场景下表现出较好的可扩展性。


### 2.2 不同实现策略的性能对比分析

在完成多种矩阵乘法实现的基础上，本项目通过系统性实验对不同实现方案的性能进行对比分析。实验主要关注以下几个方面：

* 在不同矩阵规模条件下，各实现方案的执行时间变化趋势；
* 在不同矩阵稀疏度条件下，稀疏性对计算效率和通信开销的影响；
* 不同实现方式在 Shuffle 数据量、内存使用以及任务调度开销方面的差异。

通过对实验结果的对比分析，评估各类实现方案在不同应用场景下的适用性，并揭示其性能瓶颈所在。


### 2.3 与 SystemDS 实现的对比研究

本实验采用 SystemML（SystemDS 前身）进行对比。

为进一步理解通用系统与自定义 Spark 实现之间的差异，对比 SystemML 与 Spark 自定义实现的执行时间和性能表现。

通过对比实验，分析 SystemML 的优势，并探讨在何种场景下相较于手写 Spark 程序更具性能优势。


### 2.4 分布式矩阵乘法优化策略总结

结合上述实现与实验分析结果，本项目进一步总结了在 Spark 平台上进行分布式矩阵乘法计算时的关键优化思路，包括但不限于：

* 通过分块策略减少 Shuffle 过程中传输的对象数量；
* 利用稀疏矩阵存储格式降低内存占用和无效计算；
* 在合适场景下使用广播变量以减少数据重分布；
* 关注序列化和进程间通信开销对整体性能的影响。

## 3. 实验

### 3.1 实验环境

**硬件配置**

实验在一个由 Master 和 Worker 组成的分布式集群上进行，集群硬件配置如下：

- **节点数量**：3个节点 (1 Master + 2 Workers)
    
- **CPU**：单节点 4核 Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz (Virtual Machine)
    
- **内存**：单节点 8GB RAM
    
- **存储类型**：SSD
    
- **网络带宽**：200Mbps
    

**软件环境**

- **操作系统**：Ubuntu 16.04.1 LTS (Kernel 4.4.0-210-generic)
    
- **JDK 版本**：OpenJDK 1.8.0_292
    
- **Hadoop 版本**：Hadoop 2.6.5
    
- **Spark 版本**：Spark 2.3.2
    
- **SystemML 版本**：SystemML 1.2.0
    
- **程序设计语言环境**：Python 3.5.2, Scala 2.11.12

### 3.2 实验负载

实验采用人工合成的矩阵数据集，通过`datagen.py`脚本生成，并上传至 HDFS 作为统一输入。为全面评估不同实现方案的性能，设计了如下几组实验场景：

| **场景组**       | **矩阵规模 (Rows x Cols)** | **稀疏度 (Sparsity)** | **测试目的**            |
|---------------|------------------------|--------------------|---------------------|
| **A. 稠密矩阵**   | 500, 1000, 2000        | 1.0 (Dense)        | 测试无稀疏优化下的计算基准能力     |
| **B. 规模扩展**   | 5k, 10k, 20k, 50k      | 0.01               | 测试数据量增大时的系统可扩展性     |
| **C. 稀疏度敏感性** | 5000                   | 0.001, 0.05, 0.1   | 观察稀疏度变化对计算和通信开销的影响  |
| **D. 压力测试**   | 50,000 x 50,000        | 0.01               | 极限压力测试 (约2500万非零元素) |

### 3.3 实验步骤

Step 1: 集群启动与环境检查

启动 Hadoop HDFS 与 Yarn，启动 Spark Master 与 Worker 进程，确认集群运行状态正常。

![](attachments/1766758139866%201.png)

Step 2: 数据生成

执行 run_gen.sh 脚本，在本地生成矩阵数据并上传至 HDFS /input 目录。

![](attachments/Pasted%20image%2020251226221101.png)

Step 3: 作业提交与执行

使用 spark-submit 提交 Python 和 SystemML 任务。配置参数如下：
```
$SPARK_SUBMIT \
        --name "SDS_${SCENARIO}" \
        --master spark://spark-master:7077 \
        --executor-memory 4G \
        --driver-memory 2G \
        --num-executors 3 \
        --executor-cores 3 \
        --conf spark.default.parallelism=24 \
        --conf spark.memory.fraction=0.6 \
        --conf spark.driver.maxResultSize=2g \
        $SYSTEMML_JAR \
        -f $DML_SCRIPT \
        -exec spark \
        -stats \
        -nvargs Ain="$A_PATH" Bin="$B_PATH" rows=$ROWS cols=$COLS \
        > "$LOG_FILE" 2>&1
```

  ![](attachments/Pasted%20image%2020251226221821.png)

Step 4: 性能监控 (Spark UI)

通过 Spark UI (8080/4040端口) 监控 Stage 划分与 Shuffle 情况，分析 DAG 执行流程。

![](attachments/Pasted%20image%2020251226222855.png)
![](attachments/Pasted%20image%2020251226223325.png)
![](attachments/Pasted%20image%2020251226223406.png)

### 3.4 实验结果

根据Spark UI和日志文件整理的实验数据如下表所示：

> 方法说明：
>
> * **naive**：朴素 join 实现
> * **broadcast**：广播变量实现
> * **block_opt**：分块优化实现
>

> 指标1：**执行时间（秒）**

| 实验类型     | 矩阵规模  | 稀疏度   | naive（s） | broadcast（s） | block_opt（s） | SystemML（s） |
|----------|-------|-------|----------|--------------|--------------|-------------|
| Dense    | 500   | 1.0   | 50.7426  | 2.8976       | 2.5353       | 6.373       |
| Dense    | 1000  | 1.0   | 377.5097 | 7.1202       | 5.3989       | 8.253       |
| Dense    | 2000  | 1.0   | –        | 24.7454      | 15.9631      | 15.560      |
| Scale    | 500   | 0.01  | 2.3686   | 1.5348       | 2.3932       | 5.048       |
| Scale    | 1000  | 0.01  | 2.8197   | 1.8244       | 2.3091       | 5.187       |
| Scale    | 5000  | 0.01  | 28.7116  | 47.7344      | 3.6924       | 6.387       |
| Scale    | 10000 | 0.01  | 651.8114 | 1354.9702    | 7.0715       | 11.286      |
| Scale    | 20000 | 0.01  | –        | –            | 127.0766     | 20.787      |
| Scale    | 50000 | 0.01  | –        | –            | –            | 77.451      |
| Sparsity | 5000  | 0.001 | 2.5413   | 2.2206       | 2.9230       | 5.376       |
| Sparsity | 5000  | 0.05  | 28.5842  | 54.1097      | 3.4315       | 6.719       |
| Sparsity | 5000  | 0.1   | 957.4389 | 1978.6433    | 10.7870      | 10.231      |


> 指标2：**Shuffle Read / Shuffle Write**

| 实验类型      | 矩阵规模  | 稀疏度   | naive             | broadcast         | block_opt         | SystemML          |
|-----------|-------|-------|-------------------|-------------------|-------------------|-------------------|
| Dense     | 500   | 1.0   | 141.6 MB/141.6 MB | 10.8 MB/10.8 MB   | 10.9 MB/10.9 MB   | 20.5 MB/17.1 MB   |
| Dense     | 1000  | 1.0   | 597.5 MB/597.5 MB | 56.9 MB/56.9 MB   | 44.2 MB/44.2 MB   | 83.8 MB/69.9 MB   |
| Dense     | 2000  | 1.0   | –                 | 250.9 MB/250.9 MB | 210.8 MB/210.8 MB | 361.3 MB/361.3 MB |
| Scale<br> | 500   | 0.01  | 483.8 KB/483.8 KB | 627.3 KB/627.3 KB | 260.5 KB/260.5 KB | 271.8 KB/229.2 KB |
| Scale     | 1000  | 0.01  | 2.6 MB/2.6 MB     | 3.7 MB/3.7 MB     | 1.5 MB/1.5 MB     | 1MB/876.2 KB      |
| Scale     | 5000  | 0.01  | 305.4 MB/305.4 MB | 814.8 MB/814.8 MB | 138.2 MB/138.2 MB | 31.9 MB/23.2 MB   |
| Scale     | 10000 | 0.01  | 2.4 GB/2.4 GB     | 6.5 GB/6.5 GB     | 1.1 GB/1.1 GB     | 111.7 MB/94 MB    |
| Scale     | 20000 | 0.01  | –                 | –                 | 8.2 GB/8.2 GB     | 455.1 MB/381.9 MB |
| Scale     | 50000 | 0.01  | –                 | –                 | –                 | 2.8 GB/2.5 GB     |
| Sparsity  | 5000  | 0.001 | 3.9 MB/3.9 MB     | 5 MB/5 MB         | 3.1 MB/3.1 MB     | 3.2 MB/2.8 MB     |
| Sparsity  | 5000  | 0.05  | 305.4 MB/305.4 MB | 814.8 MB/814.8 MB | 138.2 MB/138.2 MB | 27.6 MB/23.2 MB   |
| Sparsity  | 5000  | 0.1   | 6 GB/6 GB         | 16.2 GB/16.2 GB   | 1.3GB/1.3GB       | 134.4 MB/112.8 MB |

> **说明**：
> * “–” 表示OOM或Timeout未能正常完成
> * Scale = 固定稀疏度 sp = 0.01
> * Sparsity = 固定矩阵规模 5000 × 5000

#### Scala

> 指标1：**执行时间（秒）**

| 实验类型     | 矩阵规模  | 稀疏度   | block_opt（s） | block_scala（s） |
|----------|-------|-------|--------------|----------------|
| Scale    | 500   | 0.01  | 2.3932       | 8.7998         |
| Scale    | 1000  | 0.01  | 2.3091       | 8.6341         |
| Scale    | 5000  | 0.01  | 3.6924       | 12.5081        |
| Scale    | 10000 | 0.01  | 7.0715       | 25.9601        |
| Scale    | 20000 | 0.01  | 127.0766     | 238.6612       |
| Scale    | 50000 | 0.01  | –            | –              |
| Sparsity | 5000  | 0.001 | 2.9230       | 10.2159        |
| Sparsity | 5000  | 0.05  | 3.4315       | 12.4748        |

> 指标2：**Shuffle Read / Shuffle Write**

| 实验类型     | 矩阵规模  | 稀疏度   | block_opt         | block_scala       |
|----------|-------|-------|-------------------|-------------------|
| Scale    | 500   | 0.01  | 260.5 KB/260.5 KB | 442.5 KB/442.5 KB |
| Scale    | 1000  | 0.01  | 1.5 MB/1.5 MB     | 2.4 MB/2.4 MB     |
| Scale    | 5000  | 0.01  | 138.2 MB/138.2 MB | 245.6 MB/245.6 MB |
| Scale    | 10000 | 0.01  | 1.1 GB/1.1 GB     | 1.9GB/1.9GB       |
| Scale    | 20000 | 0.01  | 8.2 GB/8.2 GB     | 15 GB/15 GB       |
| Scale    | 50000 | 0.01  | –                 | –                 |
| Sparsity | 5000  | 0.001 | 3.1 MB/3.1 MB     | 4.7 MB/4.7 MB     |
| Sparsity | 5000  | 0.05  | 138.2 MB/138.2 MB | 245.6 MB/245.6 MB |


#### **分析**

从执行时间的角度分析，各实现方案的性能差异与其通信开销高度相关。朴素实现和广播实现在矩阵规模或稀疏度增大时，执行时间随 Shuffle 数据量迅速增长，表现出较差的可扩展性。基于分块策略的实现显著降低了通信和调度开销，在多数实验场景下取得了数量级的性能提升。然而，在更大规模（如 20000 及以上）时，分块实现的通信成本仍会增长至 GB 级，Python 与 JVM 之间的序列化开销开始成为瓶颈。相比之下，SystemML 在所有规模下均表现出更稳定的执行时间增长趋势，尤其在大规模稀疏矩阵计算中具有明显优势。

从 Shuffle 数据量的角度分析，不同实现方案在通信开销上的差异非常显著。朴素实现（naive）在矩阵规模或稀疏度增大时会产生大量中间结果，导致 Shuffle 数据量迅速膨胀，进而严重影响整体性能；广播实现仅在被广播矩阵规模较小时有效，在规模扩展场景下反而带来更大的通信与内存开销。 基于分块策略的实现（block）在大多数实验场景下显著降低了 Shuffle 数据量，是 Spark 自定义实现中最稳定且性能最优的方案。然而，在更大规模（如 20000 及以上）时，block 的 Shuffle 数据量仍会增长至 GB 级，Python 与 JVM 之间的序列化和通信成本开始成为瓶颈。 相比之下，SystemML 在所有规模下均能有效控制 Shuffle 数据量，尤其在大规模稀疏矩阵场景中，其通信开销显著低于 Spark 自定义实现，这也是其在超大规模实验中表现出更好稳定性和可扩展性的主要原因。


## 4. 实验结果分析与总结

### 4.1 执行时间分析

#### 4.1.1 Dense 场景下的时间表现

在稠密矩阵（Dense）场景下，朴素实现（naive）的执行时间随矩阵规模迅速增长。当矩阵规模从 500 扩展至 1000 时，执行时间由约 50 秒增长至 377 秒；在规模进一步扩大至 2000 时，朴素实现已无法在可接受时间内完成计算，表现出明显的不可扩展性。

相比之下，基于分块策略的实现（block_opt）在 Dense 场景下显著降低了执行时间，在 Dense_1000 规模下仅需约 5 秒即可完成计算，相比 naive 实现获得了数量级上的性能提升。SystemML 在稠密场景下同样保持了稳定的执行时间增长趋势，但在中小规模条件下，其执行时间略高于 block_opt，这主要来源于 SystemML 在执行计划生成与优化阶段的固定系统开销。

#### 4.1.2 Scale 场景下的时间扩展性
在固定稀疏度（sp = 0.01）条件下对矩阵规模进行扩展时，不同实现方案的可扩展性差异更加明显。朴素实现与广播实现的执行时间在规模达到 10000 时迅速增长至数百秒甚至上千秒，随后在更大规模下直接失效，表明其不适用于大规模矩阵乘法计算。

分块实现（block_opt）在中等规模（5000、10000）下表现出良好的扩展性，执行时间分别控制在 4 秒和 7 秒左右。然而，当规模增大至 20000 时，其执行时间突增至 127 秒，表明分块策略在极大规模下仍会受到通信和序列化成本的制约。

SystemML 在 Scale 场景下展现出最为平滑和稳定的时间增长曲线，即使在 50000 规模下仍能顺利完成计算。这一结果表明，SystemML 在大规模矩阵计算中具有更强的系统级优化能力和鲁棒性。

#### 4.1.3 Sparsity 场景下的时间变化
在固定矩阵规模（5000）条件下，随着稀疏度的变化，不同实现方案的执行时间表现出明显差异。在极稀疏场景（sp = 0.001）下，各方法执行时间均较短，差异不明显，此时计算主要受任务调度和固定系统开销影响。 

随着稀疏度提高至 0.05 和 0.1，朴素实现和广播实现的执行时间迅速增长，在 sp = 0.1 时分别达到近千秒和两千秒，而分块实现和 SystemML 仍能将执行时间控制在十秒量级以内，体现出其在中高稀疏度条件下的显著优势。


### 4.2 Shuffle 通信开销分析

#### 4.2.1 Shuffle 数据量与执行时间的关系

实验结果表明，不同实现方案的执行时间与其 Shuffle 通信开销高度相关。朴素实现和广播实现由于在计算过程中产生大量中间结果，导致 Shuffle 数据量随矩阵规模和稀疏度迅速膨胀，进而成为性能瓶颈。

例如，在 Scale_10000 场景下，朴素实现和广播实现的 Shuffle 数据量分别达到 2.4 GB 和 6.5 GB，对应执行时间超过 600 秒和 1300 秒；而分块实现的 Shuffle 数据量仅为 1.1 GB，其执行时间下降至 7 秒左右。

#### 4.2.2 分块策略对通信开销的优化效果

分块实现通过将元素级运算转化为块级计算，有效减少了中间结果的数量，从而显著降低 Shuffle 数据量。在多数实验场景下，block_opt 的 Shuffle 数据量仅为 naive 的 1/5 至 1/20，对应执行时间也获得了数量级的提升。

然而，在更大规模（如 Scale_20000）下，分块实现的 Shuffle 数据量仍会增长至 GB 级，通信与序列化成本开始显著影响整体性能，这也是其在该规模下出现性能拐点的主要原因。

#### 4.2.3 SystemML 的通信特性分析

SystemML 在所有实验场景中均能有效控制 Shuffle 数据量，尤其在大规模矩阵计算中，其 Shuffle 开销始终维持在 MB 级或百 MB 级。这使得 SystemML 在规模扩展时仍能保持较为稳定的执行时间增长趋势。

SystemML 的优势主要来源于其自动化的执行计划优化、算子融合以及对稀疏结构的深入利用，使其在超大规模场景下具备优于手工 Spark 实现的通信效率和整体性能。

#### 4.2.4 探究 Python 与 Scala 的性能差异
在本实验的矩阵计算场景中，Python (PySpark) 的性能反直觉地优于 Scala，且 Shuffle 数据量减少近 50%。经分析，主要原因如下：

底层计算加速 (C/Fortran)：Python 代码并未直接进行循环计算，而是通过 scipy.sparse 调用了底层高度优化的 C 语言 (BLAS/LAPACK) 线性代数库，避开了解释器开销，计算效率远超 JVM 上的 Breeze 实现。

序列化效率：Scala 的自定义矩阵对象在 JVM 中带有大量对象头（Object Header）元数据，导致 Shuffle 传输时数据严重膨胀；而 PySpark 结合 SciPy 的序列化格式更为紧凑，显著降低了 I/O 压力。

稀疏结构优化：Python 的 CSR/COO 格式在处理稀疏矩阵时能极致地过滤零元素，确保进入 Shuffle 阶段的数据绝对不含无效占位符，进一步节省了带宽。

### 4.3 小结

综合执行时间与 Shuffle 通信开销两方面的实验结果可以得出：执行时间的差异本质上源于通信开销的差异，而通信开销又由具体实现策略所决定。朴素实现和广播实现在规模或稀疏度较大时不具备可扩展性；分块实现是 Spark 自定义实现中性能最优、适用范围最广的方案；而 SystemML 在大规模矩阵乘法计算中展现出最强的稳定性和扩展能力。

## 5. 局限与展望

尽管本项目在不同规模和稀疏度条件下对多种矩阵乘法实现方案进行了系统实验和对比分析，但仍存在一定的局限性，同时也为后续工作提供了进一步优化和扩展的空间。


### 5.1 局限性

本项目中的自定义矩阵乘法实现基于 PySpark 编写，其计算逻辑运行于 Python 层，而实际执行仍依赖 JVM 调度。在大规模实验（如 Scale_20000 及以上）中，即使采用分块策略，Shuffle 数据量和执行时间仍会显著增长，这在一定程度上受到 Python 与 JVM 之间序列化、反序列化以及函数调用开销的影响。相比之下，SystemML 作为 JVM 原生系统，在超大规模场景下具备更明显的性能优势。

分块实现的性能高度依赖于块大小（block size）等参数的选择。在本实验中，块大小主要基于经验进行设置，并未对不同块大小进行系统性的参数调优。在某些规模或稀疏度条件下，当前分块参数可能并非最优配置，从而影响了性能表现。

本实验主要从执行时间和 Shuffle 通信开销两个维度评估不同实现方案的性能，而对 CPU 利用率、内存使用峰值、垃圾回收开销等更细粒度的系统指标仅进行了有限分析。此外，部分性能数据依赖 Spark Web UI 的人工整理，可能存在一定的统计误差，但整体趋势和结论不受影响。


实验主要在固定的 Spark 集群规模和资源配置下进行，尚未系统分析节点数量、Executor 配置以及资源调度策略对矩阵乘法性能的影响。


### 5.2未来展望

未来可以考虑优化 Spark 原生的 Scala 实现，或结合 JNI、C++ 等方式进一步减少 Python 层带来的运行时开销，从而提升分块矩阵乘法在超大规模场景下的性能表现。

后续工作可进一步研究基于矩阵规模、稀疏度和集群资源情况的自适应分块策略，动态调整块大小和计算方式，以避免在特定规模下出现性能拐点，从而提升分块实现的通用性和鲁棒性。

在后续实验中，可以结合 Spark EventLog、History Server 或外部监控工具，对 CPU、内存、磁盘 I/O 等系统资源进行更细粒度的监控与分析，从系统层面进一步解释不同实现方案的性能差异。

基于本实验结果，SystemML 在大规模矩阵计算中的优势已经得到验证。未来可进一步探索将自定义 Spark 实现与 SystemML 的自动优化能力相结合，或在更大规模和更复杂算子组合下对 SystemML 的优化策略进行深入分析。


### 5.3小结

总体而言，本项目验证了不同矩阵乘法实现方案在 Spark 环境下的性能特征及其适用场景。尽管自定义实现仍存在一定局限，但通过合理的优化策略已能在多数场景下获得显著性能提升。未来工作可围绕系统级优化、自适应策略以及更大规模实验展开，以进一步提升分布式矩阵计算的效率与稳定性。


## 6. 小组分工

本项目由四名成员协作完成，整体贡献度均为 25%，具体分工如下：

张航
负责项目整体统筹与实验顶层设计，制定涵盖稠密性、规模扩展性及稀疏度对比的完整测试方案。编写自动化实验脚本与数据生成模块，并参与数据分析、算法优化及项目文档内容撰写工作。

钱伟鹏
负责 Spark 分布式矩阵乘法核心算法的实现，独立完成朴素、广播与分块三种计算策略的代码开发。针对大规模矩阵计算中的性能瓶颈进行系统优化，重点改进内存管理与通信机制，保障算法在高负载与极限规模下的稳定性。

李浚宏
负责分布式集群环境的搭建以及 Spark 服务的部署与维护，为实验提供稳定可靠的运行基础。完成 SystemML 对照实验的配置与执行，用于建立性能基准，并参与核心算法优化，改进分块策略下的稀疏矩阵处理逻辑。

林宇轩
负责全量实验的执行与结果验证，利用自动化脚本完成多维参数组合下的测试任务，并系统收集运行日志与性能数据。主导实验结果的可视化分析与整理，负责项目文档的主要撰写工作，以及最终答辩 PPT 制作与演示视频剪辑。
